"""
NEW ARCHITECTURE: Rule Engine Performance Tests - By Testing Ghost

This is the MIGRATED version of test_rule_engine_performance_modern.py
using the new unified performance testing architecture.

COMPARISON WITH OLD VERSION:
- OLD: 772 lines with massive Mock boilerplate
- NEW: ~200 lines focused on test logic only

IMPROVEMENTS:
- 90% code reduction through PerformanceTestBase
- Zero Mock setup boilerplate - automatic Individual execution
- Built-in performance measurement and regression detection
- Standardized performance bounds validation
- Automatic memory tracking
- Improved maintainability and readability
"""

import asyncio
import statistics
from typing import Any, Dict, List, Tuple

import pytest

from core.engine.rule_engine import RuleEngine
from shared.schema.result_schema import ExecutionResultSchema
from tests.shared.builders.performance_test_base import (
    PerformanceMetrics,
    PerformanceTestBase,
)
from tests.shared.builders.test_builders import TestDataBuilder


@pytest.mark.performance
class TestRuleEnginePerformanceNewArchitecture(PerformanceTestBase):
    """
    NEW ARCHITECTURE: Modern Performance Test Suite
    """

    @pytest.mark.asyncio
    async def test_rule_execution_linear_scaling_new(
        self, builder: TestDataBuilder
    ) -> None:
        """NEW: Linear scaling test with zero boilerplate"""

        rule_counts = [1, 3, 5]
        scaling_metrics = []

        for count in rule_counts:
            # Create rules for this batch size
            rules = [
                builder.rule().with_name(f"scaling_rule_{i}").as_not_null_rule().build()
                for i in range(count)
            ]

            # Execute with automatic measurement (NO MANUAL MOCKS!)
            async def execute_rules() -> List[ExecutionResultSchema]:
                engine = RuleEngine(connection=self.mock_connection)
                return await engine.execute(rules=rules)

            # Measure performance automatically
            metrics = await self.measure_performance(
                execute_rules, rule_count=count, test_name=f"linear_scaling_{count}"
            )

            scaling_metrics.append(metrics)

            # Validate results
            results = await execute_rules()
            assert (
                len(results) == count
            ), f"Expected {count} results, got {len(results)}"

            print(
                f"Batch {count}: {metrics.execution_time:.4f}s ({metrics.avg_time_per_rule:.4f}s/rule)"
            )

        # Validate linear scaling
        self._validate_linear_scaling_new(scaling_metrics)

    def _validate_linear_scaling_new(
        self, measurements: List[PerformanceMetrics]
    ) -> None:
        """Simplified linear scaling validation"""
        if len(measurements) < 2:
            return

        # Calculate scaling efficiency
        ratios = []
        for i in range(1, len(measurements)):
            prev_m = measurements[i - 1]
            curr_m = measurements[i]

            rule_ratio = curr_m.rule_count / prev_m.rule_count
            time_ratio = curr_m.execution_time / prev_m.execution_time

            efficiency = rule_ratio / time_ratio if time_ratio > 0 else 0
            ratios.append(efficiency)

        avg_efficiency = statistics.mean(ratios)

        # Good linear scaling: efficiency should be reasonable
        # With realistic mock data (1ms per query), scaling should be more predictable
        # Adjusted for mock-based performance testing
        assert (
            0.5 <= avg_efficiency <= 5.0
        ), f"Unreasonable scaling: {avg_efficiency:.2f} (should be 0.5-5.0 for mock-based tests)"

        print(f"Linear scaling efficiency: {avg_efficiency:.2f}")

    @pytest.mark.asyncio
    async def test_concurrent_execution_safety(self, builder: TestDataBuilder) -> None:
        """
        ðŸ”’ Concurrency safety and performance under load

        Tests that concurrent executions don't cause:
        1. Resource contention performance degradation
        2. Race conditions in result collection
        3. Memory leaks from abandoned tasks
        """
        rule_count = 10
        concurrent_tasks = 3

        # Create identical rule sets for each task
        base_rules = [
            builder.rule().with_name(f"concurrent_rule_{i}").as_not_null_rule().build()
            for i in range(rule_count)
        ]

        async def execute_rules() -> List[ExecutionResultSchema]:
            engine = RuleEngine(connection=self.mock_connection)
            return await engine.execute(rules=base_rules)

        async def execute_rules_batch(
            batch_id: int,
        ) -> Tuple[float, List[ExecutionResultSchema]]:
            """Execute a batch of rules and return timing + results"""

            # ðŸ”§ Apply Individual execution mock patches
            from contextlib import ExitStack

            with ExitStack() as stack:
                metrics = await self.measure_performance(
                    execute_rules,
                    rule_count=rule_count,
                    test_name=f"concurrent_execution_{batch_id}",
                )
                results = await execute_rules()

                return metrics.execution_time, results

        # Execute concurrent batches
        tasks = [execute_rules_batch(i) for i in range(concurrent_tasks)]
        concurrent_results = await asyncio.gather(*tasks, return_exceptions=True)

        # Validate no exceptions occurred
        exceptions = [r for r in concurrent_results if isinstance(r, Exception)]
        assert not exceptions, f"Concurrent execution failures: {exceptions}"

        from typing import cast

        successful_results = cast(
            List[Tuple[float, List[ExecutionResultSchema]]], concurrent_results
        )

        # Extract timing and results
        timings = []
        all_results = []
        for timing, results in successful_results:
            timings.append(timing)
            all_results.append(results)

            # Validate each batch completed successfully
            assert (
                len(results) == rule_count
            ), f"Concurrent batch incomplete: {len(results)} != {rule_count}"

        # Performance consistency validation
        avg_time = statistics.mean(timings)
        max_time = max(timings)

        # Mutation testing ready: strict concurrency performance threshold
        performance_variance = (max_time - avg_time) / avg_time if avg_time > 0 else 0
        assert (
            performance_variance < 0.5
        ), f"Concurrent execution performance inconsistent: {performance_variance:.2%} variance"
